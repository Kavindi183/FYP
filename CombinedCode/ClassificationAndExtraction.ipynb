{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parser_f() got an unexpected keyword argument 'decoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d1e6c8ccc518>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new_articles_0.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: parser_f() got an unexpected keyword argument 'decoding'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('new_articles_0.csv', decoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Article_content']\n",
    "Y = df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "Y = le.fit_transform(Y.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_RandomForest = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',  RandomForestClassifier(n_estimators=200,oob_score=True,random_state=100,n_jobs=-1,max_features=None,min_samples_leaf=50))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate score for each CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores5 = cross_val_score(text_clf_RandomForest, X, Y, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997979797979798"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores5.mean()\n",
    "# 0.9979381443298969\n",
    "# 0.997979797979798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validated estimates for each input data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(text_clf_RandomForest, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...imators=200, n_jobs=-1,\n",
       "            oob_score=True, random_state=100, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using all the data\n",
    "text_clf_RandomForest.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles extracted from RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import feedparser\n",
    "# # RSS feed url\n",
    "# url = 'http://www.dailymirror.lk/RSS_Feeds/breaking-news'\n",
    "# feed = feedparser.parse(url)\n",
    "# entries = feed['entries']\n",
    "# from newspaper import Article\n",
    "# articles = []\n",
    "# for entry in entries:\n",
    "#     url = entry['link']\n",
    "#     article = Article(url)\n",
    "#     article.download()\n",
    "#     article.parse()\n",
    "#     articles.append({\n",
    "#         'Title': entry['title'].encode(\"utf-8\"),\n",
    "#         'Date': entry['published'].encode(\"utf-8\"),\n",
    "#         'Article_content': article.text.encode(\"utf-8\"),\n",
    "# #         article.nlp()\n",
    "# #         'Summary': article.summary,\n",
    "#         'Summary': entry['summary'],\n",
    "# #         'Summary': 'summary',\n",
    "#         'Link': entry['link'].encode(\"utf-8\"),\n",
    "#     })\n",
    "# import re\n",
    "# TAG_RE=re.compile(r'<[^>]+>')\n",
    "# for article in articles:\n",
    "#     summary = article['Summary']\n",
    "#     pure_summary=TAG_RE.sub('',summary)\n",
    "#     article['Summary']=pure_summary.encode(\"utf-8\")\n",
    "# import csv\n",
    "# with open('Test_dataset_content_2.csv',mode='w') as csv_file:\n",
    "#     fieldnames=['Title','Date','Article_content','Summary','Link']\n",
    "#     writer=csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "#     for article in articles:\n",
    "#         writer.writerow({'Title':article['Title'],'Date':article['Date'],'Article_content':article['Article_content'],'Summary':article['Summary'],'Link':article['Link']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "# RSS feed url\n",
    "url = 'http://www.dailymirror.lk/RSS_Feeds/breaking-news'\n",
    "feed = feedparser.parse(url)\n",
    "entries = feed['entries']\n",
    "# print(entries)\n",
    "from newspaper import Article\n",
    "articles = []\n",
    "for entry in entries:\n",
    "    url = entry['link']\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    articles.append({\n",
    "        'Title': entry['title'],\n",
    "        'Date': entry['published'],\n",
    "        'Article_content': article.text,\n",
    "#         article.nlp()\n",
    "#         'Summary': article.summary,\n",
    "        'Summary': entry['summary'],\n",
    "#         'Summary': 'summary',\n",
    "        'Link': entry['link'],\n",
    "    })\n",
    "import re\n",
    "TAG_RE=re.compile(r'<[^>]+>')\n",
    "for article in articles:\n",
    "    summary = article['Summary']\n",
    "    pure_summary=TAG_RE.sub('',summary)\n",
    "    article['Summary']=pure_summary\n",
    "import csv\n",
    "with open('Test_dataset_content_2.csv',mode='w') as csv_file:\n",
    "    fieldnames=['Title','Date','Article_content','Summary','Link']\n",
    "    writer=csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for article in articles:\n",
    "        writer.writerow({'Title':article['Title'].encode(\"utf-8\"),'Date':article['Date'].encode(\"utf-8\"),'Article_content':article['Article_content'].encode(\"utf-8\"),'Summary':article['Summary'].encode(\"utf-8\"),'Link':article['Link'].encode(\"utf-8\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('Test_dataset_content_2.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles = df_new['Article_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles = new_articles.fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "type_arr = text_clf_RandomForest.predict(new_articles)\n",
    "print(type_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mysql.connector import MySQLConnection, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'user': 'root',\n",
    "  'password': '',\n",
    "  'host': '127.0.0.1',\n",
    "  'database': 'dengue_sri_lanka',\n",
    "  'raise_on_warnings': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_article(publication_date, title, content, summary, link):\n",
    "    query = \"INSERT IGNORE INTO news_articles(publication_date, title, content, summary, link) \" \\\n",
    "            \"VALUES(%s,%s,%s,%s,%s)\"\n",
    "    args = (publication_date, title, content, summary, link)\n",
    " \n",
    "    try:\n",
    "#         db_config = read_db_config()\n",
    "        conn = MySQLConnection(**config)\n",
    " \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, args)\n",
    " \n",
    "#         if cursor.lastrowid:\n",
    "#             print('last insert id', cursor.lastrowid)\n",
    "#         else:\n",
    "#             print('last insert id not found')\n",
    " \n",
    "        conn.commit()\n",
    "    except Error as error:\n",
    "        print(error)\n",
    " \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "[0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# for articles after the test dataset\n",
    "df1 = pd.read_csv('new_articles.csv')\n",
    "prev_articles = df1['Article_Content']\n",
    "print(prev_articles.shape)\n",
    "prev_articles = prev_articles.fillna(\"\")\n",
    "type_of_prev = text_clf_RandomForest.predict(prev_articles)\n",
    "print(type_of_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDInt...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDs a...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDs\\xEF\\xBF...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDt l...' for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDs w...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDBlo...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBD\\xEF\\xBF\\xBD...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDOn ...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDs A...' for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDRs....' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDthe...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDLan...' for column 'title' at row 1\n",
      "1265: Data truncated for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBD\\xEF\\xBF\\xBD...' for column 'content' at row 1\n",
      "1265: Data truncated for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBD\\x09Th...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDBuo...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBD\\x09Dr...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDEve...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDILL...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDTou...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBD  P...' for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDs b...' for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBD \\xEF\\xBF...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDHea...' for column 'content' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDs c...' for column 'title' at row 1\n",
      "1366: Incorrect string value: '\\xEF\\xBF\\xBDIt ...' for column 'content' at row 1\n"
     ]
    }
   ],
   "source": [
    "# fix the error: Incorrect string value ..............\n",
    "# in db, make 'title' AND 'link', the unique index\n",
    "index = 0;\n",
    "for x in type_of_prev:\n",
    "    if x==0: \n",
    "        insert_article(df1.iloc[index]['Date'], df1.iloc[index]['Title'], df1.iloc[index]['Article_Content'].encode(\"utf-8\"), \"\", \"\")\n",
    "\n",
    "    #         print(type_arr.index(x))\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract articles with 0 as the result and update the db table (without duplicates)\n",
    "index = 0;\n",
    "for x in type_arr:\n",
    "    if x==0: #change to 0\n",
    "#         print(articles[index]['Title'])\n",
    "#         print(articles[index]['Title'].decode(\"utf-8\"))\n",
    "#         insert_article(articles[index]['Date'].decode(\"utf-8\"), articles[index]['Title'].decode(\"utf-8\"), articles[index]['Article_content'].decode(\"utf-8\"), articles[index]['Summary'].decode(\"utf-8\"), articles[index]['Link'].decode(\"utf-8\"))\n",
    "        insert_article(articles[index]['Date'], articles[index]['Title'], articles[index]['Article_content'], articles[index]['Summary'], articles[index]['Link'])\n",
    "\n",
    "    #         print(type_arr.index(x))\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for info extraction (use above table and extract data from that and store in respective tables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
